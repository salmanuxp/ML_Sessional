{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e73b2-a0b7-44cc-97ad-3079bd8d12af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f1f1ff50-f9de-43a1-915b-2698d32905f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries that we will be using for building the neural network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "ae7e176e-7cad-4ef7-9b69-6e7057eca084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the classes for building the neural network layers\n",
    "from tensorflow.keras.layers import Dense,Input,Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "13aa52ea-c5e8-4497-9e96-d5d7c3bf22f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialiezer=tf.keras.initializers.RandomUniform(minval=0.0005, maxval=1, seed=100)\n",
    "\n",
    "input1= Input(shape=1)\n",
    "input2= Input(shape=1)\n",
    "input3= Input(shape=1)\n",
    "input4= Input(shape=1)\n",
    "input5= Input(shape=1)\n",
    "\n",
    "l1=Dense(units=5,use_bias=True,activation='relu',kernel_initializer=initialiezer)(input1)\n",
    "l1=Dense(units=5,use_bias=True,activation='relu',kernel_initializer=initialiezer)(l1)\n",
    "\n",
    "l2=Dense(units=5,use_bias=True,activation='relu',kernel_initializer=initialiezer)(input2)\n",
    "l2=Dense(units=5,use_bias=True,activation='relu',kernel_initializer=initialiezer)(l2)\n",
    "\n",
    "concatted = Concatenate()([l1, l2])\n",
    "out=Dense(units=1,use_bias=True,activation='linear',kernel_initializer=initialiezer)(concatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "a089fad4-e2b4-4361-b9e9-def3749be0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "82ff19d6-cd40-46c9-9f89-8060d777ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model([input1,input2,input3,input4,input5],[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "4a084cfd-f78f-4f44-91ad-5c11faea1c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ac023e5a-d00e-4636-8ecf-5b01b8e7ed6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Point A</th>\n",
       "      <th>Point B</th>\n",
       "      <th>Point C</th>\n",
       "      <th>Point D</th>\n",
       "      <th>Point E</th>\n",
       "      <th>Error Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.373475</td>\n",
       "      <td>0.958978</td>\n",
       "      <td>0.836790</td>\n",
       "      <td>1.502963</td>\n",
       "      <td>0.978804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.940213</td>\n",
       "      <td>1.371291</td>\n",
       "      <td>1.743612</td>\n",
       "      <td>1.315589</td>\n",
       "      <td>2.017418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.690368</td>\n",
       "      <td>2.025028</td>\n",
       "      <td>2.025214</td>\n",
       "      <td>1.518278</td>\n",
       "      <td>1.934396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.542669</td>\n",
       "      <td>1.888478</td>\n",
       "      <td>1.641450</td>\n",
       "      <td>1.641526</td>\n",
       "      <td>2.550871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.429971</td>\n",
       "      <td>1.580948</td>\n",
       "      <td>1.820888</td>\n",
       "      <td>1.274735</td>\n",
       "      <td>1.757213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>306</td>\n",
       "      <td>0.234889</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>0.312052</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>0.359836</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>307</td>\n",
       "      <td>0.235683</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>0.340483</td>\n",
       "      <td>0.328003</td>\n",
       "      <td>0.329723</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>308</td>\n",
       "      <td>0.231840</td>\n",
       "      <td>0.313921</td>\n",
       "      <td>0.329652</td>\n",
       "      <td>0.349243</td>\n",
       "      <td>0.341359</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>309</td>\n",
       "      <td>0.237851</td>\n",
       "      <td>0.306959</td>\n",
       "      <td>0.316966</td>\n",
       "      <td>0.357427</td>\n",
       "      <td>0.358833</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>310</td>\n",
       "      <td>0.234731</td>\n",
       "      <td>0.343230</td>\n",
       "      <td>0.315228</td>\n",
       "      <td>0.321040</td>\n",
       "      <td>0.357567</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0   Point A   Point B   Point C   Point D   Point E  \\\n",
       "0             0  1.373475  0.958978  0.836790  1.502963  0.978804   \n",
       "1             1  1.940213  1.371291  1.743612  1.315589  2.017418   \n",
       "2             2  1.690368  2.025028  2.025214  1.518278  1.934396   \n",
       "3             3  1.542669  1.888478  1.641450  1.641526  2.550871   \n",
       "4             4  1.429971  1.580948  1.820888  1.274735  1.757213   \n",
       "..          ...       ...       ...       ...       ...       ...   \n",
       "306         306  0.234889  0.341797  0.312052  0.323540  0.359836   \n",
       "307         307  0.235683  0.339810  0.340483  0.328003  0.329723   \n",
       "308         308  0.231840  0.313921  0.329652  0.349243  0.341359   \n",
       "309         309  0.237851  0.306959  0.316966  0.357427  0.358833   \n",
       "310         310  0.234731  0.343230  0.315228  0.321040  0.357567   \n",
       "\n",
       "     Error Classification  \n",
       "0                       0  \n",
       "1                       1  \n",
       "2                       0  \n",
       "3                       1  \n",
       "4                       0  \n",
       "..                    ...  \n",
       "306                     4  \n",
       "307                     4  \n",
       "308                     4  \n",
       "309                     4  \n",
       "310                     4  \n",
       "\n",
       "[311 rows x 7 columns]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "inputDataFrame=pandas.read_csv(\"../dataset/dataset_with_error_cnn.csv\")\n",
    "inputDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ba155a99-8a3b-4f79-9c56-6295225794b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Point A</th>\n",
       "      <th>Point B</th>\n",
       "      <th>Point C</th>\n",
       "      <th>Point D</th>\n",
       "      <th>Point E</th>\n",
       "      <th>Error Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.373475</td>\n",
       "      <td>0.958978</td>\n",
       "      <td>0.836790</td>\n",
       "      <td>1.502963</td>\n",
       "      <td>0.978804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.940213</td>\n",
       "      <td>1.371291</td>\n",
       "      <td>1.743612</td>\n",
       "      <td>1.315589</td>\n",
       "      <td>2.017418</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.690368</td>\n",
       "      <td>2.025028</td>\n",
       "      <td>2.025214</td>\n",
       "      <td>1.518278</td>\n",
       "      <td>1.934396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.542669</td>\n",
       "      <td>1.888478</td>\n",
       "      <td>1.641450</td>\n",
       "      <td>1.641526</td>\n",
       "      <td>2.550871</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.429971</td>\n",
       "      <td>1.580948</td>\n",
       "      <td>1.820888</td>\n",
       "      <td>1.274735</td>\n",
       "      <td>1.757213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.234889</td>\n",
       "      <td>0.341797</td>\n",
       "      <td>0.312052</td>\n",
       "      <td>0.323540</td>\n",
       "      <td>0.359836</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.235683</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>0.340483</td>\n",
       "      <td>0.328003</td>\n",
       "      <td>0.329723</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.231840</td>\n",
       "      <td>0.313921</td>\n",
       "      <td>0.329652</td>\n",
       "      <td>0.349243</td>\n",
       "      <td>0.341359</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.237851</td>\n",
       "      <td>0.306959</td>\n",
       "      <td>0.316966</td>\n",
       "      <td>0.357427</td>\n",
       "      <td>0.358833</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.234731</td>\n",
       "      <td>0.343230</td>\n",
       "      <td>0.315228</td>\n",
       "      <td>0.321040</td>\n",
       "      <td>0.357567</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Point A   Point B   Point C   Point D   Point E  Error Classification\n",
       "0    1.373475  0.958978  0.836790  1.502963  0.978804                     0\n",
       "1    1.940213  1.371291  1.743612  1.315589  2.017418                     1\n",
       "2    1.690368  2.025028  2.025214  1.518278  1.934396                     0\n",
       "3    1.542669  1.888478  1.641450  1.641526  2.550871                     1\n",
       "4    1.429971  1.580948  1.820888  1.274735  1.757213                     0\n",
       "..        ...       ...       ...       ...       ...                   ...\n",
       "306  0.234889  0.341797  0.312052  0.323540  0.359836                     4\n",
       "307  0.235683  0.339810  0.340483  0.328003  0.329723                     4\n",
       "308  0.231840  0.313921  0.329652  0.349243  0.341359                     4\n",
       "309  0.237851  0.306959  0.316966  0.357427  0.358833                     4\n",
       "310  0.234731  0.343230  0.315228  0.321040  0.357567                     4\n",
       "\n",
       "[311 rows x 6 columns]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputDataFrame = inputDataFrame.drop('Unnamed: 0', axis=1)\n",
    "inputDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3cfe2-7cf3-4fbb-8908-5dc6686a0337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e2b5b411-8592-42b4-8364-97af81c4af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data out as numpy arrays\n",
    "x1s=inputDataFrame['Point A'].to_numpy()\n",
    "x2s=inputDataFrame['Point B'].to_numpy()\n",
    "x3s=inputDataFrame['Point C'].to_numpy()\n",
    "x4s=inputDataFrame['Point D'].to_numpy()\n",
    "x5s=inputDataFrame['Point E'].to_numpy()\n",
    "ys=inputDataFrame['Error Classification'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "85aab06c-7a48-461a-9095-7184fe48d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1s=x1s.reshape(x1s.shape[0],1)\n",
    "x2s=x2s.reshape(x2s.shape[0],1)\n",
    "x3s=x3s.reshape(x3s.shape[0],1)\n",
    "x4s=x4s.reshape(x4s.shape[0],1)\n",
    "x5s=x5s.reshape(x5s.shape[0],1)\n",
    "ys=ys.reshape(ys.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6a630050-4fe3-42e8-b973-c3eb84ffad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "x1Train, x1Test, x2Train, x2Test, x3Train, x3Test, x4Train, x4Test, x5Train, x5Test, yTrain, yTest = train_test_split(x1s,x2s,x3s,x4s,x5s,ys, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1ac18081-f1bd-4d1a-be97-751292af253f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of x1Train = (248, 1)\n",
      "the shape of x1Test = (63, 1)\n",
      "the shape of x2Train = (248, 1)\n",
      "the shape of x2Test = (63, 1)\n",
      "the shape of x3Train = (248, 1)\n",
      "the shape of x3Test = (63, 1)\n",
      "the shape of x4Train = (248, 1)\n",
      "the shape of x4Test = (63, 1)\n",
      "the shape of x5Train = (248, 1)\n",
      "the shape of x5Test = (63, 1)\n",
      "the shape of yTrain = (248, 1)\n",
      "the shape of yTest = (63, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'the shape of x1Train = {x1Train.shape}')\n",
    "print(f'the shape of x1Test = {x1Test.shape}')\n",
    "\n",
    "print(f'the shape of x2Train = {x5Train.shape}')\n",
    "print(f'the shape of x2Test = {x5Test.shape}')\n",
    "\n",
    "print(f'the shape of x3Train = {x5Train.shape}')\n",
    "print(f'the shape of x3Test = {x5Test.shape}')\n",
    "\n",
    "print(f'the shape of x4Train = {x5Train.shape}')\n",
    "print(f'the shape of x4Test = {x5Test.shape}')\n",
    "\n",
    "print(f'the shape of x5Train = {x5Train.shape}')\n",
    "print(f'the shape of x5Test = {x5Test.shape}')\n",
    "\n",
    "\n",
    "print(f'the shape of yTrain = {yTrain.shape}')\n",
    "print(f'the shape of yTest = {yTest.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "db9d21f5-e691-4adc-83ba-ce31dacc52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse',metrics=['mse'],optimizer=tf.optimizers.Adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f84470f7-1ccc-447c-9802-c84157b12386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "#     metrics = ['mse']\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b00456d9-f785-4399-9ff5-40fbf17d52fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 1s 876ms/step - loss: 18.8195 - mse: 18.8195 - val_loss: 15.4536 - val_mse: 15.4536\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 15.3823 - mse: 15.3823 - val_loss: 12.5846 - val_mse: 12.5846\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.6617 - mse: 12.6617 - val_loss: 10.3556 - val_mse: 10.3556\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.5562 - mse: 10.5562 - val_loss: 8.6426 - val_mse: 8.6426\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.9399 - mse: 8.9399 - val_loss: 7.3428 - val_mse: 7.3428\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.7108 - mse: 7.7108 - val_loss: 6.3762 - val_mse: 6.3762\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.7993 - mse: 6.7993 - val_loss: 5.6897 - val_mse: 5.6897\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.1521 - mse: 6.1521 - val_loss: 5.2151 - val_mse: 5.2151\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7105 - mse: 5.7105 - val_loss: 4.9090 - val_mse: 4.9090\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.4287 - mse: 5.4287 - val_loss: 4.7259 - val_mse: 4.7259\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.2644 - mse: 5.2644 - val_loss: 4.6323 - val_mse: 4.6323\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1827 - mse: 5.1827 - val_loss: 4.5917 - val_mse: 4.5917\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1499 - mse: 5.1499 - val_loss: 4.5826 - val_mse: 4.5826\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.1453 - mse: 5.1453 - val_loss: 4.5865 - val_mse: 4.5865\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1549 - mse: 5.1549 - val_loss: 4.5955 - val_mse: 4.5955\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.1681 - mse: 5.1681 - val_loss: 4.6029 - val_mse: 4.6029\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1769 - mse: 5.1769 - val_loss: 4.6044 - val_mse: 4.6044\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1776 - mse: 5.1776 - val_loss: 4.5970 - val_mse: 4.5970\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.1679 - mse: 5.1679 - val_loss: 4.5806 - val_mse: 4.5806\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.1470 - mse: 5.1470 - val_loss: 4.5536 - val_mse: 4.5536\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1151 - mse: 5.1151 - val_loss: 4.5174 - val_mse: 4.5174\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.0734 - mse: 5.0734 - val_loss: 4.4732 - val_mse: 4.4732\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.0232 - mse: 5.0232 - val_loss: 4.4220 - val_mse: 4.4220\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.9655 - mse: 4.9655 - val_loss: 4.3650 - val_mse: 4.3650\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.9016 - mse: 4.9016 - val_loss: 4.3035 - val_mse: 4.3035\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.8327 - mse: 4.8327 - val_loss: 4.2386 - val_mse: 4.2386\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.7599 - mse: 4.7599 - val_loss: 4.1713 - val_mse: 4.1713\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.6844 - mse: 4.6844 - val_loss: 4.1026 - val_mse: 4.1026\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.6069 - mse: 4.6069 - val_loss: 4.0335 - val_mse: 4.0335\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.5286 - mse: 4.5286 - val_loss: 3.9648 - val_mse: 3.9648\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4505 - mse: 4.4505 - val_loss: 3.8973 - val_mse: 3.8973\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.3732 - mse: 4.3732 - val_loss: 3.8316 - val_mse: 3.8316\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.2975 - mse: 4.2975 - val_loss: 3.7683 - val_mse: 3.7683\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.2242 - mse: 4.2242 - val_loss: 3.7078 - val_mse: 3.7078\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.1535 - mse: 4.1535 - val_loss: 3.6504 - val_mse: 3.6504\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0859 - mse: 4.0859 - val_loss: 3.5964 - val_mse: 3.5964\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.0216 - mse: 4.0216 - val_loss: 3.5458 - val_mse: 3.5458\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.9607 - mse: 3.9607 - val_loss: 3.4986 - val_mse: 3.4986\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.9033 - mse: 3.9033 - val_loss: 3.4548 - val_mse: 3.4548\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.8492 - mse: 3.8492 - val_loss: 3.4140 - val_mse: 3.4140\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.7984 - mse: 3.7984 - val_loss: 3.3762 - val_mse: 3.3762\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.7505 - mse: 3.7505 - val_loss: 3.3408 - val_mse: 3.3408\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.7054 - mse: 3.7054 - val_loss: 3.3077 - val_mse: 3.3077\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.6626 - mse: 3.6626 - val_loss: 3.2764 - val_mse: 3.2764\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.6219 - mse: 3.6219 - val_loss: 3.2465 - val_mse: 3.2465\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.5829 - mse: 3.5829 - val_loss: 3.2178 - val_mse: 3.2178\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.5452 - mse: 3.5452 - val_loss: 3.1897 - val_mse: 3.1897\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5086 - mse: 3.5086 - val_loss: 3.1621 - val_mse: 3.1621\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.4728 - mse: 3.4728 - val_loss: 3.1348 - val_mse: 3.1348\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.4374 - mse: 3.4374 - val_loss: 3.1074 - val_mse: 3.1074\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.4024 - mse: 3.4024 - val_loss: 3.0800 - val_mse: 3.0800\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3676 - mse: 3.3676 - val_loss: 3.0523 - val_mse: 3.0523\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.3329 - mse: 3.3329 - val_loss: 3.0244 - val_mse: 3.0244\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.2983 - mse: 3.2983 - val_loss: 2.9963 - val_mse: 2.9963\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2637 - mse: 3.2637 - val_loss: 2.9680 - val_mse: 2.9680\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2291 - mse: 3.2291 - val_loss: 2.9396 - val_mse: 2.9396\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.1946 - mse: 3.1946 - val_loss: 2.9112 - val_mse: 2.9112\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.1602 - mse: 3.1602 - val_loss: 2.8828 - val_mse: 2.8828\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.1260 - mse: 3.1260 - val_loss: 2.8543 - val_mse: 2.8543\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0923 - mse: 3.0923 - val_loss: 2.8259 - val_mse: 2.8259\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.0592 - mse: 3.0592 - val_loss: 2.7976 - val_mse: 2.7976\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0271 - mse: 3.0271 - val_loss: 2.7698 - val_mse: 2.7698\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.9964 - mse: 2.9964 - val_loss: 2.7428 - val_mse: 2.7428\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9670 - mse: 2.9670 - val_loss: 2.7173 - val_mse: 2.7173\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9385 - mse: 2.9385 - val_loss: 2.6933 - val_mse: 2.6933\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.9109 - mse: 2.9109 - val_loss: 2.6705 - val_mse: 2.6705\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8842 - mse: 2.8842 - val_loss: 2.6484 - val_mse: 2.6484\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8581 - mse: 2.8581 - val_loss: 2.6270 - val_mse: 2.6270\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8328 - mse: 2.8328 - val_loss: 2.6061 - val_mse: 2.6061\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8081 - mse: 2.8081 - val_loss: 2.5861 - val_mse: 2.5861\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7839 - mse: 2.7839 - val_loss: 2.5669 - val_mse: 2.5669\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7603 - mse: 2.7603 - val_loss: 2.5483 - val_mse: 2.5483\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7373 - mse: 2.7373 - val_loss: 2.5303 - val_mse: 2.5303\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7151 - mse: 2.7151 - val_loss: 2.5130 - val_mse: 2.5130\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6934 - mse: 2.6934 - val_loss: 2.4960 - val_mse: 2.4960\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6724 - mse: 2.6724 - val_loss: 2.4795 - val_mse: 2.4795\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6521 - mse: 2.6521 - val_loss: 2.4640 - val_mse: 2.4640\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6329 - mse: 2.6329 - val_loss: 2.4500 - val_mse: 2.4500\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6146 - mse: 2.6146 - val_loss: 2.4365 - val_mse: 2.4365\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.5970 - mse: 2.5970 - val_loss: 2.4236 - val_mse: 2.4236\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5797 - mse: 2.5797 - val_loss: 2.4109 - val_mse: 2.4109\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5629 - mse: 2.5629 - val_loss: 2.3985 - val_mse: 2.3985\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.5465 - mse: 2.5465 - val_loss: 2.3864 - val_mse: 2.3864\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5304 - mse: 2.5304 - val_loss: 2.3745 - val_mse: 2.3745\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5147 - mse: 2.5147 - val_loss: 2.3630 - val_mse: 2.3630\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4993 - mse: 2.4993 - val_loss: 2.3515 - val_mse: 2.3515\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.4841 - mse: 2.4841 - val_loss: 2.3402 - val_mse: 2.3402\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4692 - mse: 2.4692 - val_loss: 2.3289 - val_mse: 2.3289\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4545 - mse: 2.4545 - val_loss: 2.3177 - val_mse: 2.3177\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4400 - mse: 2.4400 - val_loss: 2.3065 - val_mse: 2.3065\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4256 - mse: 2.4256 - val_loss: 2.2954 - val_mse: 2.2954\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4113 - mse: 2.4113 - val_loss: 2.2843 - val_mse: 2.2843\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3972 - mse: 2.3972 - val_loss: 2.2731 - val_mse: 2.2731\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.3832 - mse: 2.3832 - val_loss: 2.2620 - val_mse: 2.2620\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3694 - mse: 2.3694 - val_loss: 2.2510 - val_mse: 2.2510\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3557 - mse: 2.3557 - val_loss: 2.2400 - val_mse: 2.2400\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.3422 - mse: 2.3422 - val_loss: 2.2291 - val_mse: 2.2291\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.3289 - mse: 2.3289 - val_loss: 2.2183 - val_mse: 2.2183\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.3156 - mse: 2.3156 - val_loss: 2.2075 - val_mse: 2.2075\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3026 - mse: 2.3026 - val_loss: 2.1970 - val_mse: 2.1970\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2898 - mse: 2.2898 - val_loss: 2.1865 - val_mse: 2.1865\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.2772 - mse: 2.2772 - val_loss: 2.1760 - val_mse: 2.1760\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2646 - mse: 2.2646 - val_loss: 2.1655 - val_mse: 2.1655\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2521 - mse: 2.2521 - val_loss: 2.1551 - val_mse: 2.1551\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.2396 - mse: 2.2396 - val_loss: 2.1448 - val_mse: 2.1448\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2272 - mse: 2.2272 - val_loss: 2.1345 - val_mse: 2.1345\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2149 - mse: 2.2149 - val_loss: 2.1242 - val_mse: 2.1242\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2026 - mse: 2.2026 - val_loss: 2.1141 - val_mse: 2.1141\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.1903 - mse: 2.1903 - val_loss: 2.1039 - val_mse: 2.1039\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1782 - mse: 2.1782 - val_loss: 2.0939 - val_mse: 2.0939\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1662 - mse: 2.1662 - val_loss: 2.0841 - val_mse: 2.0841\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1544 - mse: 2.1544 - val_loss: 2.0743 - val_mse: 2.0743\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1427 - mse: 2.1427 - val_loss: 2.0646 - val_mse: 2.0646\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1310 - mse: 2.1310 - val_loss: 2.0550 - val_mse: 2.0550\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1195 - mse: 2.1195 - val_loss: 2.0454 - val_mse: 2.0454\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1080 - mse: 2.1080 - val_loss: 2.0359 - val_mse: 2.0359\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0965 - mse: 2.0965 - val_loss: 2.0265 - val_mse: 2.0265\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0851 - mse: 2.0851 - val_loss: 2.0170 - val_mse: 2.0170\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.0737 - mse: 2.0737 - val_loss: 2.0077 - val_mse: 2.0077\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0624 - mse: 2.0624 - val_loss: 1.9984 - val_mse: 1.9984\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0511 - mse: 2.0511 - val_loss: 1.9892 - val_mse: 1.9892\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0399 - mse: 2.0399 - val_loss: 1.9800 - val_mse: 1.9800\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0287 - mse: 2.0287 - val_loss: 1.9710 - val_mse: 1.9710\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0176 - mse: 2.0176 - val_loss: 1.9619 - val_mse: 1.9619\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0065 - mse: 2.0065 - val_loss: 1.9530 - val_mse: 1.9530\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9955 - mse: 1.9955 - val_loss: 1.9441 - val_mse: 1.9441\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9846 - mse: 1.9846 - val_loss: 1.9353 - val_mse: 1.9353\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9736 - mse: 1.9736 - val_loss: 1.9265 - val_mse: 1.9265\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.9627 - mse: 1.9627 - val_loss: 1.9177 - val_mse: 1.9177\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9517 - mse: 1.9517 - val_loss: 1.9090 - val_mse: 1.9090\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9408 - mse: 1.9408 - val_loss: 1.9003 - val_mse: 1.9003\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.9299 - mse: 1.9299 - val_loss: 1.8917 - val_mse: 1.8917\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9190 - mse: 1.9190 - val_loss: 1.8831 - val_mse: 1.8831\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9081 - mse: 1.9081 - val_loss: 1.8745 - val_mse: 1.8745\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8973 - mse: 1.8973 - val_loss: 1.8659 - val_mse: 1.8659\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.8864 - mse: 1.8864 - val_loss: 1.8573 - val_mse: 1.8573\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.8755 - mse: 1.8755 - val_loss: 1.8487 - val_mse: 1.8487\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.8646 - mse: 1.8646 - val_loss: 1.8401 - val_mse: 1.8401\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.8537 - mse: 1.8537 - val_loss: 1.8315 - val_mse: 1.8315\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8428 - mse: 1.8428 - val_loss: 1.8229 - val_mse: 1.8229\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8319 - mse: 1.8319 - val_loss: 1.8143 - val_mse: 1.8143\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8209 - mse: 1.8209 - val_loss: 1.8056 - val_mse: 1.8056\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8099 - mse: 1.8099 - val_loss: 1.7969 - val_mse: 1.7969\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7989 - mse: 1.7989 - val_loss: 1.7882 - val_mse: 1.7882\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7878 - mse: 1.7878 - val_loss: 1.7795 - val_mse: 1.7795\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7768 - mse: 1.7768 - val_loss: 1.7707 - val_mse: 1.7707\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7656 - mse: 1.7656 - val_loss: 1.7619 - val_mse: 1.7619\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7545 - mse: 1.7545 - val_loss: 1.7531 - val_mse: 1.7531\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.7433 - mse: 1.7433 - val_loss: 1.7443 - val_mse: 1.7443\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.7320 - mse: 1.7320 - val_loss: 1.7353 - val_mse: 1.7353\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7207 - mse: 1.7207 - val_loss: 1.7263 - val_mse: 1.7263\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7094 - mse: 1.7094 - val_loss: 1.7173 - val_mse: 1.7173\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6980 - mse: 1.6980 - val_loss: 1.7082 - val_mse: 1.7082\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6865 - mse: 1.6865 - val_loss: 1.6990 - val_mse: 1.6990\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6750 - mse: 1.6750 - val_loss: 1.6898 - val_mse: 1.6898\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6635 - mse: 1.6635 - val_loss: 1.6806 - val_mse: 1.6806\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6519 - mse: 1.6519 - val_loss: 1.6713 - val_mse: 1.6713\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6403 - mse: 1.6403 - val_loss: 1.6620 - val_mse: 1.6620\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6286 - mse: 1.6286 - val_loss: 1.6528 - val_mse: 1.6528\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6169 - mse: 1.6169 - val_loss: 1.6436 - val_mse: 1.6436\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6052 - mse: 1.6052 - val_loss: 1.6344 - val_mse: 1.6344\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5935 - mse: 1.5935 - val_loss: 1.6252 - val_mse: 1.6252\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5818 - mse: 1.5818 - val_loss: 1.6161 - val_mse: 1.6161\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5700 - mse: 1.5700 - val_loss: 1.6070 - val_mse: 1.6070\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.5582 - mse: 1.5582 - val_loss: 1.5979 - val_mse: 1.5979\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5464 - mse: 1.5464 - val_loss: 1.5888 - val_mse: 1.5888\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5345 - mse: 1.5345 - val_loss: 1.5797 - val_mse: 1.5797\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5227 - mse: 1.5227 - val_loss: 1.5707 - val_mse: 1.5707\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5108 - mse: 1.5108 - val_loss: 1.5617 - val_mse: 1.5617\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4990 - mse: 1.4990 - val_loss: 1.5528 - val_mse: 1.5528\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4872 - mse: 1.4872 - val_loss: 1.5440 - val_mse: 1.5440\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4753 - mse: 1.4753 - val_loss: 1.5353 - val_mse: 1.5353\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4636 - mse: 1.4636 - val_loss: 1.5269 - val_mse: 1.5269\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4520 - mse: 1.4520 - val_loss: 1.5188 - val_mse: 1.5188\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4407 - mse: 1.4407 - val_loss: 1.5108 - val_mse: 1.5108\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4295 - mse: 1.4295 - val_loss: 1.5031 - val_mse: 1.5031\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4186 - mse: 1.4186 - val_loss: 1.4955 - val_mse: 1.4955\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4078 - mse: 1.4078 - val_loss: 1.4881 - val_mse: 1.4881\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3970 - mse: 1.3970 - val_loss: 1.4808 - val_mse: 1.4808\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3864 - mse: 1.3864 - val_loss: 1.4736 - val_mse: 1.4736\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3759 - mse: 1.3759 - val_loss: 1.4665 - val_mse: 1.4665\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3657 - mse: 1.3657 - val_loss: 1.4596 - val_mse: 1.4596\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3556 - mse: 1.3556 - val_loss: 1.4529 - val_mse: 1.4529\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.3457 - mse: 1.3457 - val_loss: 1.4462 - val_mse: 1.4462\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3359 - mse: 1.3359 - val_loss: 1.4397 - val_mse: 1.4397\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3262 - mse: 1.3262 - val_loss: 1.4332 - val_mse: 1.4332\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3165 - mse: 1.3165 - val_loss: 1.4269 - val_mse: 1.4269\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3070 - mse: 1.3070 - val_loss: 1.4208 - val_mse: 1.4208\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2975 - mse: 1.2975 - val_loss: 1.4147 - val_mse: 1.4147\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2882 - mse: 1.2882 - val_loss: 1.4088 - val_mse: 1.4088\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2790 - mse: 1.2790 - val_loss: 1.4031 - val_mse: 1.4031\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2700 - mse: 1.2700 - val_loss: 1.3976 - val_mse: 1.3976\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2610 - mse: 1.2610 - val_loss: 1.3922 - val_mse: 1.3922\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2523 - mse: 1.2523 - val_loss: 1.3870 - val_mse: 1.3870\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2437 - mse: 1.2437 - val_loss: 1.3820 - val_mse: 1.3820\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2353 - mse: 1.2353 - val_loss: 1.3772 - val_mse: 1.3772\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2271 - mse: 1.2271 - val_loss: 1.3726 - val_mse: 1.3726\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2190 - mse: 1.2190 - val_loss: 1.3682 - val_mse: 1.3682\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2112 - mse: 1.2112 - val_loss: 1.3640 - val_mse: 1.3640\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2036 - mse: 1.2036 - val_loss: 1.3600 - val_mse: 1.3600\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1962 - mse: 1.1962 - val_loss: 1.3562 - val_mse: 1.3562\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1889 - mse: 1.1889 - val_loss: 1.3526 - val_mse: 1.3526\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1819 - mse: 1.1819 - val_loss: 1.3491 - val_mse: 1.3491\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1750 - mse: 1.1750 - val_loss: 1.3459 - val_mse: 1.3459\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1684 - mse: 1.1684 - val_loss: 1.3429 - val_mse: 1.3429\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1619 - mse: 1.1619 - val_loss: 1.3400 - val_mse: 1.3400\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1556 - mse: 1.1556 - val_loss: 1.3372 - val_mse: 1.3372\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1495 - mse: 1.1495 - val_loss: 1.3346 - val_mse: 1.3346\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1436 - mse: 1.1436 - val_loss: 1.3321 - val_mse: 1.3321\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1378 - mse: 1.1378 - val_loss: 1.3297 - val_mse: 1.3297\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1323 - mse: 1.1323 - val_loss: 1.3274 - val_mse: 1.3274\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1269 - mse: 1.1269 - val_loss: 1.3252 - val_mse: 1.3252\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1217 - mse: 1.1217 - val_loss: 1.3232 - val_mse: 1.3232\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1167 - mse: 1.1167 - val_loss: 1.3212 - val_mse: 1.3212\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1119 - mse: 1.1119 - val_loss: 1.3193 - val_mse: 1.3193\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1073 - mse: 1.1073 - val_loss: 1.3175 - val_mse: 1.3175\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1028 - mse: 1.1028 - val_loss: 1.3159 - val_mse: 1.3159\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0985 - mse: 1.0985 - val_loss: 1.3144 - val_mse: 1.3144\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0944 - mse: 1.0944 - val_loss: 1.3130 - val_mse: 1.3130\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0904 - mse: 1.0904 - val_loss: 1.3118 - val_mse: 1.3118\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0866 - mse: 1.0866 - val_loss: 1.3108 - val_mse: 1.3108\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0830 - mse: 1.0830 - val_loss: 1.3099 - val_mse: 1.3099\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0795 - mse: 1.0795 - val_loss: 1.3090 - val_mse: 1.3090\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0762 - mse: 1.0762 - val_loss: 1.3082 - val_mse: 1.3082\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0731 - mse: 1.0731 - val_loss: 1.3074 - val_mse: 1.3074\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0701 - mse: 1.0701 - val_loss: 1.3067 - val_mse: 1.3067\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0672 - mse: 1.0672 - val_loss: 1.3059 - val_mse: 1.3059\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0644 - mse: 1.0644 - val_loss: 1.3053 - val_mse: 1.3053\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0618 - mse: 1.0618 - val_loss: 1.3048 - val_mse: 1.3048\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0593 - mse: 1.0593 - val_loss: 1.3045 - val_mse: 1.3045\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0569 - mse: 1.0569 - val_loss: 1.3042 - val_mse: 1.3042\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0546 - mse: 1.0546 - val_loss: 1.3040 - val_mse: 1.3040\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0525 - mse: 1.0525 - val_loss: 1.3039 - val_mse: 1.3039\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0504 - mse: 1.0504 - val_loss: 1.3039 - val_mse: 1.3039\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0484 - mse: 1.0484 - val_loss: 1.3038 - val_mse: 1.3038\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0466 - mse: 1.0466 - val_loss: 1.3038 - val_mse: 1.3038\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0448 - mse: 1.0448 - val_loss: 1.3038 - val_mse: 1.3038\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0431 - mse: 1.0431 - val_loss: 1.3039 - val_mse: 1.3039\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0416 - mse: 1.0416 - val_loss: 1.3039 - val_mse: 1.3039\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0401 - mse: 1.0401 - val_loss: 1.3040 - val_mse: 1.3040\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0387 - mse: 1.0387 - val_loss: 1.3040 - val_mse: 1.3040\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0373 - mse: 1.0373 - val_loss: 1.3041 - val_mse: 1.3041\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0361 - mse: 1.0361 - val_loss: 1.3042 - val_mse: 1.3042\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0349 - mse: 1.0349 - val_loss: 1.3044 - val_mse: 1.3044\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0337 - mse: 1.0337 - val_loss: 1.3045 - val_mse: 1.3045\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0326 - mse: 1.0326 - val_loss: 1.3047 - val_mse: 1.3047\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0316 - mse: 1.0316 - val_loss: 1.3048 - val_mse: 1.3048\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0306 - mse: 1.0306 - val_loss: 1.3050 - val_mse: 1.3050\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0296 - mse: 1.0296 - val_loss: 1.3051 - val_mse: 1.3051\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0287 - mse: 1.0287 - val_loss: 1.3053 - val_mse: 1.3053\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0278 - mse: 1.0278 - val_loss: 1.3054 - val_mse: 1.3054\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0270 - mse: 1.0270 - val_loss: 1.3055 - val_mse: 1.3055\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0263 - mse: 1.0263 - val_loss: 1.3056 - val_mse: 1.3056\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0255 - mse: 1.0255 - val_loss: 1.3057 - val_mse: 1.3057\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0248 - mse: 1.0248 - val_loss: 1.3058 - val_mse: 1.3058\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0241 - mse: 1.0241 - val_loss: 1.3059 - val_mse: 1.3059\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0234 - mse: 1.0234 - val_loss: 1.3060 - val_mse: 1.3060\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0228 - mse: 1.0228 - val_loss: 1.3061 - val_mse: 1.3061\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0222 - mse: 1.0222 - val_loss: 1.3062 - val_mse: 1.3062\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0216 - mse: 1.0216 - val_loss: 1.3063 - val_mse: 1.3063\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0210 - mse: 1.0210 - val_loss: 1.3064 - val_mse: 1.3064\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0204 - mse: 1.0204 - val_loss: 1.3064 - val_mse: 1.3064\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0199 - mse: 1.0199 - val_loss: 1.3065 - val_mse: 1.3065\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0193 - mse: 1.0193 - val_loss: 1.3065 - val_mse: 1.3065\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0188 - mse: 1.0188 - val_loss: 1.3065 - val_mse: 1.3065\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0183 - mse: 1.0183 - val_loss: 1.3065 - val_mse: 1.3065\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0178 - mse: 1.0178 - val_loss: 1.3065 - val_mse: 1.3065\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0173 - mse: 1.0173 - val_loss: 1.3064 - val_mse: 1.3064\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0168 - mse: 1.0168 - val_loss: 1.3064 - val_mse: 1.3064\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0163 - mse: 1.0163 - val_loss: 1.3063 - val_mse: 1.3063\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0158 - mse: 1.0158 - val_loss: 1.3062 - val_mse: 1.3062\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0153 - mse: 1.0153 - val_loss: 1.3061 - val_mse: 1.3061\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0148 - mse: 1.0148 - val_loss: 1.3060 - val_mse: 1.3060\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0143 - mse: 1.0143 - val_loss: 1.3058 - val_mse: 1.3058\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0138 - mse: 1.0138 - val_loss: 1.3057 - val_mse: 1.3057\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0134 - mse: 1.0134 - val_loss: 1.3055 - val_mse: 1.3055\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0129 - mse: 1.0129 - val_loss: 1.3052 - val_mse: 1.3052\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0124 - mse: 1.0124 - val_loss: 1.3050 - val_mse: 1.3050\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0119 - mse: 1.0119 - val_loss: 1.3047 - val_mse: 1.3047\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0115 - mse: 1.0115 - val_loss: 1.3044 - val_mse: 1.3044\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0110 - mse: 1.0110 - val_loss: 1.3041 - val_mse: 1.3041\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0105 - mse: 1.0105 - val_loss: 1.3037 - val_mse: 1.3037\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0100 - mse: 1.0100 - val_loss: 1.3034 - val_mse: 1.3034\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0095 - mse: 1.0095 - val_loss: 1.3030 - val_mse: 1.3030\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0090 - mse: 1.0090 - val_loss: 1.3026 - val_mse: 1.3026\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0086 - mse: 1.0086 - val_loss: 1.3021 - val_mse: 1.3021\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0081 - mse: 1.0081 - val_loss: 1.3017 - val_mse: 1.3017\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0076 - mse: 1.0076 - val_loss: 1.3013 - val_mse: 1.3013\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0071 - mse: 1.0071 - val_loss: 1.3008 - val_mse: 1.3008\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0066 - mse: 1.0066 - val_loss: 1.3003 - val_mse: 1.3003\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0061 - mse: 1.0061 - val_loss: 1.2998 - val_mse: 1.2998\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0056 - mse: 1.0056 - val_loss: 1.2994 - val_mse: 1.2994\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0051 - mse: 1.0051 - val_loss: 1.2988 - val_mse: 1.2988\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0046 - mse: 1.0046 - val_loss: 1.2983 - val_mse: 1.2983\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0040 - mse: 1.0040 - val_loss: 1.2978 - val_mse: 1.2978\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0035 - mse: 1.0035 - val_loss: 1.2972 - val_mse: 1.2972\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0030 - mse: 1.0030 - val_loss: 1.2967 - val_mse: 1.2967\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0025 - mse: 1.0025 - val_loss: 1.2961 - val_mse: 1.2961\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0019 - mse: 1.0019 - val_loss: 1.2955 - val_mse: 1.2955\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0014 - mse: 1.0014 - val_loss: 1.2949 - val_mse: 1.2949\n"
     ]
    }
   ],
   "source": [
    "# train the model. just run a few epochs for this test run. you can adjust later.\n",
    "history=model.fit(x=[x1Train,x2Train,x3Train,x4Train,x5Train],y=[yTrain],validation_data=([x1Test,x2Test,x3Test,x4Test,x5Test],[yTest]),batch_size=256, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867bfdb-5f5a-4a28-a7ca-4a191107e620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3153ab31-8e5a-462e-8be0-29494749d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "input=[x1Test,x2Test,x3Test,x4Test,x5Test]\n",
    "output=model.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "df4d2249-859d-4f12-851b-7939dd17bb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.2189012 ],\n",
       "        [0.4311295 ],\n",
       "        [2.06381969],\n",
       "        [1.26644033],\n",
       "        [0.41536098],\n",
       "        [1.23810714],\n",
       "        [2.39391574],\n",
       "        [2.44197207],\n",
       "        [0.30718794],\n",
       "        [1.053414  ],\n",
       "        [1.35731942],\n",
       "        [2.74618095],\n",
       "        [0.26405766],\n",
       "        [0.23183988],\n",
       "        [0.24236677],\n",
       "        [0.24489725],\n",
       "        [2.21387761],\n",
       "        [1.54596281],\n",
       "        [2.78380361],\n",
       "        [3.11619779],\n",
       "        [1.34123904],\n",
       "        [2.4747303 ],\n",
       "        [2.03693165],\n",
       "        [2.2647886 ],\n",
       "        [1.38547947],\n",
       "        [2.87443667],\n",
       "        [2.50913767],\n",
       "        [1.70493721],\n",
       "        [0.27514546],\n",
       "        [0.2173051 ],\n",
       "        [2.48369847],\n",
       "        [1.5426692 ],\n",
       "        [2.8351269 ],\n",
       "        [0.26688441],\n",
       "        [2.44866664],\n",
       "        [2.40354072],\n",
       "        [0.97357666],\n",
       "        [1.57939083],\n",
       "        [1.88917669],\n",
       "        [2.54388982],\n",
       "        [2.7258265 ],\n",
       "        [1.22067571],\n",
       "        [2.62847327],\n",
       "        [0.26576658],\n",
       "        [0.28758335],\n",
       "        [1.81157884],\n",
       "        [2.81372337],\n",
       "        [2.14468361],\n",
       "        [0.22165609],\n",
       "        [1.39448417],\n",
       "        [2.54997765],\n",
       "        [0.27756998],\n",
       "        [1.72768787],\n",
       "        [2.56070877],\n",
       "        [2.07081112],\n",
       "        [2.92078507],\n",
       "        [0.24574243],\n",
       "        [1.95879247],\n",
       "        [1.76578055],\n",
       "        [2.39456893],\n",
       "        [2.78251585],\n",
       "        [2.04836142],\n",
       "        [2.22408487]]),\n",
       " array([[0.2972011 ],\n",
       "        [0.63093332],\n",
       "        [1.54141348],\n",
       "        [0.86312874],\n",
       "        [0.4308853 ],\n",
       "        [0.79330999],\n",
       "        [2.8679315 ],\n",
       "        [2.21720129],\n",
       "        [0.3299798 ],\n",
       "        [1.59937956],\n",
       "        [0.88446864],\n",
       "        [2.82180776],\n",
       "        [0.37186431],\n",
       "        [0.31392145],\n",
       "        [0.31273502],\n",
       "        [0.31317213],\n",
       "        [1.66472167],\n",
       "        [2.34074279],\n",
       "        [2.56430424],\n",
       "        [2.70760439],\n",
       "        [0.86701957],\n",
       "        [2.78829938],\n",
       "        [2.30920705],\n",
       "        [2.18483747],\n",
       "        [0.91451174],\n",
       "        [1.9551493 ],\n",
       "        [2.77472564],\n",
       "        [2.19994417],\n",
       "        [0.3575359 ],\n",
       "        [0.3191558 ],\n",
       "        [2.78798316],\n",
       "        [1.88847822],\n",
       "        [2.69063182],\n",
       "        [0.22221596],\n",
       "        [2.84353708],\n",
       "        [2.66156563],\n",
       "        [1.51610413],\n",
       "        [2.04395212],\n",
       "        [1.89680874],\n",
       "        [2.81039996],\n",
       "        [2.83743641],\n",
       "        [1.66646659],\n",
       "        [2.37918828],\n",
       "        [0.31723802],\n",
       "        [0.37522426],\n",
       "        [1.97180904],\n",
       "        [2.95542187],\n",
       "        [1.49912215],\n",
       "        [0.28912344],\n",
       "        [0.92491482],\n",
       "        [2.81558243],\n",
       "        [0.38548368],\n",
       "        [2.10670206],\n",
       "        [2.66029221],\n",
       "        [1.79519326],\n",
       "        [3.03380136],\n",
       "        [0.33343423],\n",
       "        [1.83419161],\n",
       "        [2.31532526],\n",
       "        [2.56510307],\n",
       "        [2.98232124],\n",
       "        [2.17056023],\n",
       "        [2.42368735]]),\n",
       " array([[0.31011922],\n",
       "        [0.44421555],\n",
       "        [1.25007585],\n",
       "        [1.02534737],\n",
       "        [0.54467609],\n",
       "        [0.88537014],\n",
       "        [1.77785791],\n",
       "        [2.02762381],\n",
       "        [0.35566314],\n",
       "        [0.98189775],\n",
       "        [1.21739597],\n",
       "        [2.87370111],\n",
       "        [0.34825814],\n",
       "        [0.32965167],\n",
       "        [0.26502361],\n",
       "        [0.34661545],\n",
       "        [1.61084314],\n",
       "        [2.53027752],\n",
       "        [2.7711092 ],\n",
       "        [2.64991151],\n",
       "        [1.1577576 ],\n",
       "        [1.8706454 ],\n",
       "        [1.47827885],\n",
       "        [2.79785966],\n",
       "        [1.31601637],\n",
       "        [2.18326721],\n",
       "        [1.66689014],\n",
       "        [1.72645038],\n",
       "        [0.37366546],\n",
       "        [0.28276797],\n",
       "        [1.8110067 ],\n",
       "        [1.6414499 ],\n",
       "        [1.99852489],\n",
       "        [0.21444239],\n",
       "        [1.84498509],\n",
       "        [1.84202979],\n",
       "        [0.99766369],\n",
       "        [1.24676633],\n",
       "        [2.85242199],\n",
       "        [1.69942403],\n",
       "        [2.78322244],\n",
       "        [1.14196973],\n",
       "        [2.51500171],\n",
       "        [0.2777095 ],\n",
       "        [0.32058296],\n",
       "        [2.81649383],\n",
       "        [2.44639488],\n",
       "        [1.45102267],\n",
       "        [0.28381674],\n",
       "        [1.35540768],\n",
       "        [1.70871122],\n",
       "        [0.27264394],\n",
       "        [1.5109259 ],\n",
       "        [1.92107397],\n",
       "        [1.48690124],\n",
       "        [2.49818592],\n",
       "        [0.28961599],\n",
       "        [2.88705995],\n",
       "        [2.48577526],\n",
       "        [2.80885586],\n",
       "        [2.37517084],\n",
       "        [2.9035698 ],\n",
       "        [1.62407521]]),\n",
       " array([[0.32434102],\n",
       "        [0.71426009],\n",
       "        [1.43802759],\n",
       "        [0.86312874],\n",
       "        [0.33459158],\n",
       "        [0.79330999],\n",
       "        [1.83986749],\n",
       "        [2.37650275],\n",
       "        [0.44676098],\n",
       "        [1.053414  ],\n",
       "        [0.88446864],\n",
       "        [2.86564588],\n",
       "        [0.29570047],\n",
       "        [0.34924283],\n",
       "        [0.31137622],\n",
       "        [0.35872992],\n",
       "        [1.77937006],\n",
       "        [2.2111891 ],\n",
       "        [2.70198282],\n",
       "        [2.31172179],\n",
       "        [0.86701957],\n",
       "        [1.82662445],\n",
       "        [1.42285457],\n",
       "        [2.34451488],\n",
       "        [0.91451174],\n",
       "        [2.38472164],\n",
       "        [1.75213814],\n",
       "        [1.5244611 ],\n",
       "        [0.34433767],\n",
       "        [0.33474191],\n",
       "        [1.79265655],\n",
       "        [1.64152602],\n",
       "        [2.04463977],\n",
       "        [0.36192707],\n",
       "        [1.86066605],\n",
       "        [1.809283  ],\n",
       "        [0.97357666],\n",
       "        [1.21775358],\n",
       "        [1.41489354],\n",
       "        [1.7389164 ],\n",
       "        [2.4510737 ],\n",
       "        [0.9928752 ],\n",
       "        [2.32917049],\n",
       "        [0.39437328],\n",
       "        [0.30930193],\n",
       "        [1.36670713],\n",
       "        [2.77203478],\n",
       "        [1.61017744],\n",
       "        [0.31931095],\n",
       "        [0.92491482],\n",
       "        [1.74275471],\n",
       "        [0.35693762],\n",
       "        [1.74712384],\n",
       "        [1.96254186],\n",
       "        [2.95088286],\n",
       "        [2.69390003],\n",
       "        [0.34644896],\n",
       "        [1.49797137],\n",
       "        [1.54778495],\n",
       "        [3.27216878],\n",
       "        [2.83734284],\n",
       "        [2.39522329],\n",
       "        [1.77438591]]),\n",
       " array([[0.34569713],\n",
       "        [0.39918521],\n",
       "        [1.64086425],\n",
       "        [2.01702481],\n",
       "        [0.43115615],\n",
       "        [2.04501825],\n",
       "        [2.06076024],\n",
       "        [1.70767808],\n",
       "        [0.57281089],\n",
       "        [1.3640971 ],\n",
       "        [2.0755145 ],\n",
       "        [2.01243694],\n",
       "        [0.35056974],\n",
       "        [0.34135894],\n",
       "        [0.39582589],\n",
       "        [0.32871476],\n",
       "        [2.05979194],\n",
       "        [1.79100503],\n",
       "        [1.98356574],\n",
       "        [2.30593044],\n",
       "        [2.06685467],\n",
       "        [1.97905974],\n",
       "        [2.24776187],\n",
       "        [1.6719716 ],\n",
       "        [2.08588865],\n",
       "        [2.18888092],\n",
       "        [1.82895715],\n",
       "        [2.18775064],\n",
       "        [0.30194144],\n",
       "        [0.33226435],\n",
       "        [1.94473699],\n",
       "        [2.55087113],\n",
       "        [2.25383181],\n",
       "        [0.32967172],\n",
       "        [2.05197981],\n",
       "        [1.96947263],\n",
       "        [1.42473041],\n",
       "        [2.12523459],\n",
       "        [1.35142856],\n",
       "        [1.80428632],\n",
       "        [2.47733738],\n",
       "        [1.94089504],\n",
       "        [1.89390006],\n",
       "        [0.38830641],\n",
       "        [0.38480921],\n",
       "        [1.34176787],\n",
       "        [2.02710994],\n",
       "        [2.24572771],\n",
       "        [0.36619713],\n",
       "        [2.09546911],\n",
       "        [1.7984649 ],\n",
       "        [0.42585425],\n",
       "        [1.27690586],\n",
       "        [2.16930212],\n",
       "        [1.38574438],\n",
       "        [2.18620743],\n",
       "        [0.37857825],\n",
       "        [1.39286979],\n",
       "        [1.55584634],\n",
       "        [2.0802317 ],\n",
       "        [2.01688581],\n",
       "        [1.90625973],\n",
       "        [2.15186331]])]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "0de24587-b53e-4627-bc72-6366cb18980c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4],\n",
       "       [4],\n",
       "       [0],\n",
       "       [1],\n",
       "       [4],\n",
       "       [1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [0],\n",
       "       [1],\n",
       "       [3],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [4],\n",
       "       [1],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [1],\n",
       "       [3],\n",
       "       [2],\n",
       "       [2],\n",
       "       [4],\n",
       "       [4],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [3],\n",
       "       [2],\n",
       "       [3],\n",
       "       [0],\n",
       "       [3],\n",
       "       [4],\n",
       "       [4],\n",
       "       [3],\n",
       "       [3],\n",
       "       [1],\n",
       "       [4],\n",
       "       [1],\n",
       "       [2],\n",
       "       [4],\n",
       "       [2],\n",
       "       [2],\n",
       "       [3],\n",
       "       [3],\n",
       "       [4],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [3],\n",
       "       [2]], dtype=int64)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "5cda7262-07b1-4e04-be79-15e42d7df51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.5271053],\n",
       "       [3.433547 ],\n",
       "       [2.0927858],\n",
       "       [2.5615242],\n",
       "       [3.3588505],\n",
       "       [2.5622437],\n",
       "       [2.393181 ],\n",
       "       [2.043774 ],\n",
       "       [3.438776 ],\n",
       "       [3.1545124],\n",
       "       [2.4665093],\n",
       "       [2.0327463],\n",
       "       [3.508901 ],\n",
       "       [3.5197122],\n",
       "       [3.5068066],\n",
       "       [3.504041 ],\n",
       "       [2.0058494],\n",
       "       [2.9634607],\n",
       "       [1.8765785],\n",
       "       [1.6235138],\n",
       "       [2.4755635],\n",
       "       [2.2783222],\n",
       "       [2.476364 ],\n",
       "       [2.19918  ],\n",
       "       [2.4534106],\n",
       "       [1.5055736],\n",
       "       [2.238892 ],\n",
       "       [2.7449007],\n",
       "       [3.4892144],\n",
       "       [3.539207 ],\n",
       "       [2.269546 ],\n",
       "       [2.7559247],\n",
       "       [1.8860502],\n",
       "       [3.4358644],\n",
       "       [2.3291352],\n",
       "       [2.2877765],\n",
       "       [3.209401 ],\n",
       "       [2.793025 ],\n",
       "       [2.4264   ],\n",
       "       [2.2220745],\n",
       "       [2.0596125],\n",
       "       [2.9894934],\n",
       "       [1.9397917],\n",
       "       [3.4814458],\n",
       "       [3.48286  ],\n",
       "       [2.536006 ],\n",
       "       [2.0300074],\n",
       "       [1.9952762],\n",
       "       [3.5201092],\n",
       "       [2.449593 ],\n",
       "       [2.2186308],\n",
       "       [3.4993901],\n",
       "       [2.6795702],\n",
       "       [2.1359577],\n",
       "       [2.2042918],\n",
       "       [1.9635099],\n",
       "       [3.512489 ],\n",
       "       [2.3302438],\n",
       "       [2.7401128],\n",
       "       [2.2514682],\n",
       "       [2.0725675],\n",
       "       [2.4007719],\n",
       "       [2.3496218]], dtype=float32)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "612cab4d-430c-4091-b20e-68b73b0ffc59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int64)"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "8ed0a748-d799-4f4a-885f-c97607ccbcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output= np.rint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "f4eec67b-4241-4916-8ce2-2a71ae8784e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 1)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "9c83082c-f402-4e04-86d2-c87e6b269e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 1)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "53c30196-b9a7-4bfd-bdfd-6ec60b3cc7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36507936507936506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(yTest, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91532f-bbbf-4fef-a9d4-610014246b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlpds",
   "language": "python",
   "name": "mlpds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
